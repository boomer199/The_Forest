{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Predicting Stock Movement Using Random Forest</h1>\n",
    "Repo: https://github.com/areed1192/sigma_coding_youtube/blob/master/python/python-data-science/machine-learning/random-forest/random_forest_price_prediction.ipynb\n",
    "\n",
    "\n",
    "<h2>Random Forest</h2>\n",
    "A random forest is an ensemble learning method that combines the predictions of multiple decision trees to improve accuracy and robustness. Each tree contributes a vote, and the final decision is made based on the majority vote from all the (decision) trees.\n",
    "\n",
    "<h2>Decision Trees</h2>\n",
    "A decision tree is a model used to make predictions by splitting data into branches based on feature values. Each branch represents a decision rule, leading to further splits until a final prediction is made at the leaf nodes. This method is intuitive and visually interpretable, making it useful for understanding complex decision-making processes.\n",
    "\n",
    "<ul>\n",
    "    <li>Root Node: The root node is the topmost node of a decision tree, representing the entire dataset and the starting point for splitting data.</li>\n",
    "    <li>Splitting: Splitting is the process of dividing a node into two or more sub-nodes based on specific conditions.</li>\n",
    "    <li>Decision Node: A decision node is an intermediate node where decisions are made based on feature values, leading to further splits.</li>\n",
    "    <li>Leaf Node: A leaf node, or terminal node, is the endpoint of a branch representing the final output or decision.</li>\n",
    "    <li>Parent/Child Node: A parent node splits into sub-nodes called child nodes, organizing the tree structure and defining decision paths.</li>\n",
    "    <li>Pruning: Pruning involves removing unnecessary nodes to prevent overfitting and improve the model's generalization.</li>\n",
    "    <li>Branching/Sub-Tree: Branching refers to dividing a node into sub-nodes, while a sub-tree is a smaller section of the decision tree starting from a particular node.</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "\n",
    "<h2>Why a Random Forest?</h2>\n",
    "Even small changes to the input data can **dramatically** alter the overall structure of a decision tree, making it unstable. Decision trees are often relatively inaccurate, with many other predictors performing better on similar data. For data including categorical variables with different numbers of levels, information gain in decision trees is biased in favor of attributes with more levels. Calculations can become very complex, particularly if many values are uncertain and/or if many outcomes are linked. These are some of the reasons it's preferable to use Random Forest, as it helps overcome some of the weaknesses of decision trees. However, no model is perfect. \n",
    "\n",
    "<h2>Bagging</h2>\n",
    "Bagging, short for bootstrap aggregating, is a technique used in random forests to improve the stability and accuracy of machine learning models. It involves generating multiple subsets of the original dataset through random sampling with replacement. Each subset is used to train a separate decision tree. The final prediction of the random forest is obtained by averaging the predictions (for regression) or taking a majority vote (for classification) from all the individual trees. This process reduces variance and helps prevent overfitting, leading to more robust and reliable predictions.\n",
    "\n",
    "<h2>Supervised Learning</h2>\n",
    "Random Forest uses supervised learning. In supervised learning, we provide the model with a \"LABELED\" data set which tells the model what the \"correct\" value it should be. Random Forest, is an example of a supervised learning algorithim because we provide the model a labeled data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Preprocessing</h2>\n",
    " This section of the code is responsible for data preprocessing.\n",
    " It includes steps to clean, transform, and prepare the data for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import roc_curve, RocCurveDisplay\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Grabbing data from Yahoo Finance</h3>\n",
    "Downloads daily price history for a list of tickers and saves it to a CSV file. You need to run this every day to get the newest price data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_price_data():\n",
    "\n",
    "    # Define the list of tickers\n",
    "    tickers_list = ['JPM', 'COST', 'IBM', 'HD', 'ARWR'] #JPMorgan, Costco, IBM, Home Depot, Arrowhead Pharmaceuticals\n",
    "    \n",
    "    # I need to store multiple result sets.\n",
    "    full_price_history = []\n",
    "\n",
    "    for ticker in tickers_list:\n",
    "\n",
    "        # Grab the daily price history for 2 years\n",
    "        stock_data = yf.download(ticker, period='2y', interval='1d')\n",
    "\n",
    "        # Reset index to make 'Date' a column instead of the index\n",
    "        stock_data.reset_index(inplace=True)\n",
    "\n",
    "        # Rename the 'Date' column to 'datetime' to match the expected column names\n",
    "        stock_data.rename(columns={'Date': 'datetime', 'Open': 'open', 'High': 'high', 'Low': 'low', 'Close': 'close', 'Volume': 'volume'}, inplace=True)\n",
    "\n",
    "        # Add a 'symbol' column to the dataframe\n",
    "        stock_data['symbol'] = ticker\n",
    "\n",
    "        # Append the dataframe to the list\n",
    "        full_price_history.append(stock_data)\n",
    "\n",
    "    # Concatenate all dataframes in the list into a single dataframe\n",
    "    all_data = pd.concat(full_price_history)\n",
    "\n",
    "    # Dump the data to a CSV file, don't have an index column\n",
    "    all_data.to_csv('price_data.csv', index=False)\n",
    "\n",
    "# Run the function to grab the data\n",
    "grab_price_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('price_data.csv'):\n",
    "    # Load the data\n",
    "    price_data = pd.read_csv('price_data.csv')\n",
    "else:\n",
    "    # Grab the data and store it.\n",
    "    grab_price_data()\n",
    "    # Load the data\n",
    "    price_data = pd.read_csv('price_data.csv')\n",
    "\n",
    "# Display the head before moving on.\n",
    "price_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Reformatting the Data</h2>\n",
    "Process the price data by selecting specific columns, sorting the data, calculating change in price, and handling null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "price_data = price_data[[\"symbol\", \"datetime\", \"close\", \"high\", \"low\", \"open\", \"volume\"]]\n",
    "\n",
    "#Sort the data by symbol and datetime\n",
    "price_data.sort_values(by=['symbol', 'datetime'], inplace=True) #inplace=true means that the changes are saved to the dataframe and we dont need another variable to store the changes\n",
    "\n",
    "price_data[\"change_in_price\"] = price_data[\"close\"].diff()\n",
    "\n",
    "#identify the rows where the ticker symbol changes, and then shift the change_in_price value to NaN\n",
    "mask = price_data[\"symbol\"] != price_data[\"symbol\"].shift(1)\n",
    "\n",
    "#change change_in_price to NaN where needed\n",
    "price_data[\"change_in_price\"] = np.where(mask == True, np.NaN, price_data[\"change_in_price\"]) #this is a numpy function that takes 3 arguments, the first is the condition, the second is the value to be used if the\n",
    "                                                                                              #condition is true, and the third is the value to be used if the condition is false\n",
    "                                                                                              \n",
    "#see all null vals                                                                                              \n",
    "price_data[price_data.isna().any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Calculating Indicators</h2>\n",
    "RSI: The Relative Strength Index (RSI) is a momentum oscillator used in technical analysis to measure the speed and change of price movements. It ranges from 0 to 100 and is typically used to identify overbought or oversold conditions in a market, with values above 70 indicating overbought conditions and values below 30 indicating oversold conditions. RSI helps traders make decisions about potential price reversals or continuation trends.\n",
    "\n",
    "RSI = 100 + (100/(1+RS) where RS = (Avg. Gain/Avg. Loss)\n",
    "\n",
    "\n",
    "<h4>Indicator Code</h4>\n",
    "A lot of the same steps are applied to calculate the indicator that we will be using. Generally, the code follows this format:\n",
    "\n",
    "<ol>\n",
    "    <li>Copy the desired columns and store them as variables.</li>\n",
    "    <li>Group columns by symbol, select column we want to transform, and use the transform method anf a lambda function to calculate the indicator.</li>\n",
    "    <li>Store values back in main data frame.</li>\n",
    "</ol>\n",
    "\n",
    "Note: Slight variations will exist, however these are negligible enough not to be mentioned here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 14 day RSI\n",
    "n = 14\n",
    "\n",
    "# First make a copy of the data frame twice (exact same both times)\n",
    "up_df, down_df = price_data[['symbol','change_in_price']].copy(), price_data[['symbol','change_in_price']].copy()\n",
    "\n",
    "# For up days, if the change is less than 0 set to 0. This will only contain positive values.\n",
    "up_df.loc['change_in_price'] = up_df.loc[(up_df['change_in_price'] < 0), 'change_in_price'] = 0\n",
    "\n",
    "# For down days, if the change is greater than 0 set to 0. This will only contain negative values.\n",
    "down_df.loc['change_in_price'] = down_df.loc[(down_df['change_in_price'] > 0), 'change_in_price'] = 0\n",
    "\n",
    "# We need change in price to be absolute, so we use abs to make it so. (Only for down days)\n",
    "down_df['change_in_price'] = down_df['change_in_price'].abs()\n",
    "\n",
    "# Calculate the EWMA (Exponential Weighted Moving Average), meaning older values are given less weight compared to newer values.\n",
    "ewma_up = up_df.groupby('symbol')['change_in_price'].transform(lambda x: x.ewm(span = n).mean()) #The ewm function is exponential weighted moving average over a specified span of n days\n",
    "ewma_down = down_df.groupby('symbol')['change_in_price'].transform(lambda x: x.ewm(span = n).mean()) #The groupby makes it so the ewm is done by symbol and not over the whole dataset\n",
    "\n",
    "\n",
    "# Calculate the Relative Strength\n",
    "relative_strength = ewma_up / ewma_down\n",
    "\n",
    "# Calculate the Relative Strength Index\n",
    "relative_strength_index = 100.0 - (100.0 / (1.0 + relative_strength))\n",
    "\n",
    "# Add the info to the data frame.\n",
    "price_data['down_days'] = down_df['change_in_price']\n",
    "price_data['up_days'] = up_df['change_in_price']\n",
    "price_data['RSI'] = relative_strength_index\n",
    "\n",
    "# Display the head.\n",
    "price_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Stochastic Oscillator</h2>\n",
    "\n",
    "The stochastic oscillator is a momentum indicator that compares a particular closing price of a security to a range of its prices over a specific period, typically 14 days. It helps identify overbought and oversold conditions by measuring the location of the closing price relative to the high-low range over that period.\n",
    "\n",
    "%K=(Highest High − Lowest Low/Current Close − Lowest Low)×100\n",
    "\n",
    "Again, the steps here are nearly identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Stochastic Oscillator\n",
    "n = 14\n",
    "\n",
    "# Make a copy of the high and low column. (Instead of change in price like we did with RSI)\n",
    "low_14, high_14 = price_data[['symbol','low']].copy(), price_data[['symbol','high']].copy()\n",
    "\n",
    "# Group by symbol, then apply the rolling function and grab the Min and Max.\n",
    "low_14 = low_14.groupby('symbol')['low'].transform(lambda x: x.rolling(window = n).min()) #Pandas dataframe.rolling() function provides the feature of rolling window calculations.\n",
    "high_14 = high_14.groupby('symbol')['high'].transform(lambda x: x.rolling(window = n).max())\n",
    "\n",
    "# Calculate the Stochastic Oscillator.\n",
    "k_percent = 100 * ((price_data['close'] - low_14) / (high_14 - low_14))\n",
    "\n",
    "# Add the info to the data frame.\n",
    "price_data['low_14'] = low_14\n",
    "price_data['high_14'] = high_14\n",
    "price_data['k_percent'] = k_percent\n",
    "\n",
    "# Display the head.\n",
    "price_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Williams %R</h2>\n",
    "\n",
    "Williams %R ranges from -100 to 0. When its value is above -20, it indicates a sell signal and when its value is below -80, it indicates a buy signal. It is nearly identical in formula to the Stochastic Oscillator. \n",
    "\n",
    "%R=( Highest High − Lowest Low/ Highest High − Current Close)×−100\n",
    "\n",
    "Again, the steps here are nearly identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Williams %R. Until r_percent is calculated, the code is IDENTICAL to the Stochastic Oscillator.\n",
    "n = 14\n",
    "\n",
    "# Make a copy of the high and low column.\n",
    "low_14, high_14 = price_data[['symbol','low']].copy(), price_data[['symbol','high']].copy()\n",
    "\n",
    "# Group by symbol, then apply the rolling function and grab the Min and Max.\n",
    "low_14 = low_14.groupby('symbol')['low'].transform(lambda x: x.rolling(window = n).min())\n",
    "high_14 = high_14.groupby('symbol')['high'].transform(lambda x: x.rolling(window = n).max())\n",
    "\n",
    "# Calculate William %R indicator.\n",
    "r_percent = ((high_14 - price_data['close']) / (high_14 - low_14)) * - 100\n",
    "\n",
    "# Add the info to the data frame. \n",
    "price_data['r_percent'] = r_percent\n",
    "\n",
    "# Display the head.\n",
    "price_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Moving Average Convergence Divergence</h2>\n",
    "\n",
    "The Moving Average Convergence Divergence (MACD) is a trend-following momentum indicator that shows the relationship between two moving averages of a security's price. It is used to identify potential buy and sell signals based on the convergence and divergence of these moving averages.\n",
    "\n",
    "MACD Line:\n",
    "MACD Line=EMA_12(c)−EMA_26(c) where C = closing price\n",
    "\n",
    "Signal Line:\n",
    "Signal Line=EMA_9(MACD)\n",
    "\n",
    "\n",
    "Again, the steps here are nearly identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the MACD\n",
    "ema_26 = price_data.groupby('symbol')['close'].transform(lambda x: x.ewm(span = 26).mean())\n",
    "ema_12 = price_data.groupby('symbol')['close'].transform(lambda x: x.ewm(span = 12).mean())\n",
    "macd = ema_12 - ema_26\n",
    "\n",
    "# Calculate the EMA\n",
    "ema_9_macd = macd.ewm(span = 9).mean()\n",
    "\n",
    "# Store the data in the data frame.\n",
    "price_data['MACD'] = macd\n",
    "price_data['MACD_EMA'] = ema_9_macd\n",
    "\n",
    "# Print the head.\n",
    "price_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Price Rate of Change</h2>\n",
    "\n",
    "The Price Rate of Change (ROC) is a momentum oscillator that measures the percentage change in price between the current price and the price a certain number of periods ago. It helps identify the strength of price movements and potential reversals.\n",
    "\n",
    "ROC=(Price n-Periods Ago / Current Price − Price n-Periods Ago)x100\n",
    "\n",
    "Again, the steps here are nearly identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Price Rate of Change\n",
    "n = 9\n",
    "\n",
    "# Calculate the Rate of Change in the Price, and store it in the Data Frame.\n",
    "price_data['Price_Rate_Of_Change'] = price_data.groupby('symbol')['close'].transform(lambda x: x.pct_change(periods = n)) #pct_change computes the fractional change from the immediately previous row by default. This is useful in comparing the fraction of change in a time series of elements.\n",
    "\n",
    "# Print the first 30 rows\n",
    "price_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>On Balance Volume</h2>\n",
    "\n",
    "Although similar, this will be harder to calculate. \n",
    "\n",
    "The On-Balance Volume (OBV) is a cumulative momentum indicator that measures buying and selling pressure by adding the volume on up days and subtracting the volume on down days. It helps confirm trends and potential reversal points in a security's price.\n",
    "\n",
    "Formula (I dont know how do do piecewise in markdown):\n",
    "\n",
    "def calculate_obv(close_prices, volumes):\n",
    "    obv = [0]  # Start with an initial OBV value of 0\n",
    "    \n",
    "    for i in range(1, len(close_prices)):\n",
    "        if close_prices[i] > close_prices[i - 1]:\n",
    "            obv.append(obv[-1] + volumes[i])\n",
    "        elif close_prices[i] < close_prices[i - 1]:\n",
    "            obv.append(obv[-1] - volumes[i])\n",
    "        else:\n",
    "            obv.append(obv[-1])\n",
    "    \n",
    "    return obv\n",
    "\n",
    "This portion involves using the apply method to calculate the On Balance Volume by applying a custom function to each row. The function calculates the difference in closing prices and adjusts the volume: adding if the price increased, subtracting if it decreased, and leaving it unchanged if the price remained the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obv(group):\n",
    "    \"\"\"\n",
    "    Calculate the On Balance Volume (OBV) for a given group of data.\n",
    "\n",
    "    Parameters:\n",
    "    - group (pandas.DataFrame): A pandas DataFrame containing the 'volume' and 'close' columns.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.Series: A pandas Series containing the calculated OBV values.\n",
    "\n",
    "    Example:\n",
    "    >>> data = pd.DataFrame({'volume': [100, 200, 150, 300], 'close': [10, 12, 11, 13]})\n",
    "    >>> obv(data)\n",
    "    0    100\n",
    "    1    300\n",
    "    2    150\n",
    "    3    450\n",
    "    dtype: int64\n",
    "    \"\"\"\n",
    "    # Grab the volume and close column.\n",
    "    volume = group['volume']\n",
    "    change = group['close'].diff()\n",
    "\n",
    "    # intialize the previous OBV\n",
    "    prev_obv = 0\n",
    "    obv_values = []\n",
    "\n",
    "    # calculate the On Balance Volume\n",
    "    for i, j in zip(change, volume):\n",
    "\n",
    "        if i > 0:\n",
    "            current_obv = prev_obv + j\n",
    "        elif i < 0:\n",
    "            current_obv = prev_obv - j\n",
    "        else:\n",
    "            current_obv = prev_obv\n",
    "\n",
    "        # OBV.append(current_OBV)\n",
    "        prev_obv = current_obv\n",
    "        obv_values.append(current_obv)\n",
    "    \n",
    "    # Return a panda series.\n",
    "    return pd.Series(obv_values, index = group.index)\n",
    "        \n",
    "\n",
    "# apply the function to each group\n",
    "obv_groups = price_data.groupby('symbol').apply(obv)\n",
    "\n",
    "# add to the data frame, but drop the old index, before adding it.\n",
    "price_data['On Balance Volume'] = obv_groups.reset_index(level=0, drop=True)\n",
    "\n",
    "# display the data frame.\n",
    "price_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Creating the Prediction Column</h2>\n",
    "\n",
    "Now that we have our technical indicators calculated and our price data cleaned up, we are almost ready to build our model. However, we are missing one critical piece of information: the column we wish to predict. Our goal is to predict whether the next day will be a down day or an up day, making this a classification problem with two discrete groups.\n",
    "\n",
    "To create our prediction column, we will group our DataFrame by each symbol and select the close column to determine if the stock closed up or down for any given day. We will use the transform method to apply a lambda function that uses the shift() function to return True on up days and False on down days. We then multiply those results by 1 so that all False turn into 0s and all True turn into 1s. We then add this to our Prediction Column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closed_groups = price_data.groupby('symbol')['close']\n",
    "\n",
    "closed_groups = closed_groups.transform(lambda x: x.shift(1) < x)\n",
    "\n",
    "price_data[\"Prediction\"] = closed_groups * 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Removing NaN Values</h2>\n",
    "\n",
    "The random forest can't accept NaN values, so we will need to remove them before feeding the data in. The code below prints the number of rows before dropping the NaN values, use the dropna method to remove any rows NaN values and then displays the number of rows after dropping the NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to remove all rows that have an NaN value.\n",
    "print('Before NaN Drop we have {} rows and {} columns'.format(price_data.shape[0], price_data.shape[1]))\n",
    "\n",
    "# Any row that has a `NaN` value will be dropped.\n",
    "price_data = price_data.dropna()\n",
    "\n",
    "# Display how much we have left now.\n",
    "print('After NaN Drop we have {} rows and {} columns'.format(price_data.shape[0], price_data.shape[1]))\n",
    "\n",
    "# Print the head.\n",
    "price_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Selecting Indicators to Use in our Model</h2>\n",
    "\n",
    "We need to identify our input columns which are the following:\n",
    "<ul>\n",
    "    <li>RSI</li>\n",
    "    <li>Stochastic Oscillator</li>\n",
    "    <li>William %R</li>\n",
    "    <li>Price Rate of Change</li>\n",
    "    <li>MACD</li>\n",
    "    <li>On Balance Volume</li>\n",
    "</ul>\n",
    "\n",
    "These will serve as our x, and our Y column will be the prediction column.\n",
    "\n",
    "Once we've selected our columns, we need to split the data into a training and test set. SciKit learn makes this easy by providing the train_test_split object, which will take our X_Cols and Y_Cols and split them based on the size we input. In our case, let's have the test_size be '20 %. For reproducibility, the train_test_splitobject provides therandom_state` argument that will split the data along the same dimensions every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab our X & Y Columns.\n",
    "X_Cols = price_data[['RSI','k_percent','r_percent','Price_Rate_Of_Change','MACD','On Balance Volume']]\n",
    "Y_Cols = price_data['Prediction']\n",
    "\n",
    "# Split X and y into X_\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_Cols, Y_Cols, random_state = 0)\n",
    "\n",
    "# Create a Random Forest Classifier (using xgb boost increased success rate by 2%)\n",
    "rand_frst_clf = XGBClassifier(n_estimators=10, max_depth=2, learning_rate=1, objective='binary:logistic') \n",
    "\n",
    "# Fit the data to the model\n",
    "rand_frst_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rand_frst_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Accuracy</h2>\n",
    "\n",
    "SciKit learn makes the process of evaluating our model very easy by providing a bunch of built-in metrics that we can call.\n",
    "\n",
    "One of those metrics is the accuracy_score.\n",
    "\n",
    "The accuracy_score function computes the accuracy, either the fraction (default) or the count (normalize=False) of correct predictions. Accuracy is defined as the number of accurate predictions the model made on the test set. Imagine we had three TRUE values [1, 2, 3], and our model predicted the following values [1, 2, 4] we would say the accuracy of our model is 66 %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Correct Prediction (%): ', accuracy_score(y_test, rand_frst_clf.predict(X_test), normalize = True) * 100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Classification Report</h2>\n",
    "\n",
    "Accuracy:\n",
    "Accuracy measures the portion of all testing samples classified correctly and is defined as the following:\n",
    "\\[ \\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}} \\]\n",
    "\n",
    "Recall:\n",
    "Recall (also known as sensitivity) measures the ability of a classifier to correctly identify positive labels and is defined as the following:\n",
    "\\[ \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} \\]\n",
    "The recall is intuitively the ability of the classifier to find all the positive samples. The best value is 1, and the worst value is 0.\n",
    "\n",
    "Specificity:\n",
    "Specificity measures the classifier’s ability to correctly identify negative labels and is defined as the following:\n",
    "\\[ \\text{Specificity} = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}} \\]\n",
    "\n",
    "Precision:\n",
    "Precision measures the proportion of all correctly identified samples in a population of samples which are classified as positive labels and is defined as the following:\n",
    "\\[ \\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} \\]\n",
    "The precision is intuitively the ability of the classifier not to label as positive a sample that is negative. The best value is 1, and the worst value is 0.\n",
    "\n",
    "Interpreting the Classification Report:\n",
    "When it comes to evaluating the model, we generally look at the accuracy. If our accuracy is high, it means our model is correctly classifying items. In some cases, we will have models that may have low precision or high recall. It's difficult to compare two models with low precision and high recall or vice versa. To make results comparable, we use a metric called the F-Score. The F-score helps to measure Recall and Precision at the same time. It uses Harmonic Mean in place of Arithmetic Mean by punishing the extreme values more.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the traget names\n",
    "target_names = ['Down Day', 'Up Day']\n",
    "\n",
    "# Build a classifcation report\n",
    "report = classification_report(y_true = y_test, y_pred = y_pred, target_names = target_names, output_dict = True)\n",
    "\n",
    "# Add it to a data frame, transpose it for readability. (transpose is a pandas function that swaps the rows and columns)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "report_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Feature Importance</h2>\n",
    "\n",
    "Feature importance is a technique used in machine learning to determine the relative significance of each feature in a predictive model. It helps in identifying which features contribute the most to the model’s predictions, allowing for better model interpretation and potential feature selection. In tree-based models like Random Forests, feature importance is typically calculated by measuring the decrease in node impurity (e.g., Gini impurity or Accuracy) for each feature across all trees in the model. Features with higher importance scores are more influential in predicting the target variable.\n",
    "\n",
    "\n",
    "\n",
    "<h4>Calculating the Feature Importance</h4>\n",
    "\n",
    "Like all the previous steps, SkLearn makes this process very easy. Take your rand_frst_clf and call the feature_importances_ property. This will return all of our features and their importance measurement. Store the values in a Pandas.Series object and sore the values.\n",
    "\n",
    "Feature importance can be calculated two ways in Random Forest:\n",
    "\n",
    "Gini-Based Importance\n",
    "Accuracy-Based Importance\n",
    "\n",
    "Here is how both measures of importance are calculated.\n",
    "\n",
    "With sklearn they use the Gini-Importance metric for the Random Forest Algorithm.\n",
    "\n",
    "We can see in our model, that the most important feature is k_percent and our least important feature is OBV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importance and store in pandas series\n",
    "feature_imp = pd.Series(rand_frst_clf.feature_importances_, index=X_Cols.columns).sort_values(ascending=False)\n",
    "feature_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>ROC Curve</h2>\n",
    "\n",
    "The greater the area under the curve, the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_disp = RocCurveDisplay.from_estimator(rand_frst_clf, X_test, y_test)\n",
    "rfc_disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Improvements</h2>\n",
    "\n",
    "To determine the optimal number of estimators for a Random Forest, you need to try different values rather than relying on a fixed number. This can be achieved using the RandomizedSearchCV method from sklearn, which tests a wide range of possible values for each hyperparameter using cross-validation to find the best combination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [10, 100, 200],\n",
    "    'max_depth': [1,2,3, 4, 5],\n",
    "    'learning_rate': [0.01, 0.1, 0.2,0.3,0.5,0.7,0.8,1],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'reg_alpha': [0, 0.1, 0.5],\n",
    "    'reg_lambda': [1, 1.5, 2]\n",
    "}\n",
    "xgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Running Randomized Search</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search = RandomizedSearchCV(estimator=xgb_clf, param_distributions=param_grid, n_iter=50, scoring='accuracy', cv=3, verbose=1, n_jobs=-1, random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "print(\"Best accuracy score: \", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Testing Improved RF</h2>\n",
    "\n",
    "Print new tests then print optimal best estimations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test set accuracy: \", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
